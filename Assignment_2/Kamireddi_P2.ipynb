{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happiest Citizens in the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Regression technique - Linear Regression, is applied to create a model that can predict the ranking as close as possible to the actual results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SparkSession libraries.\n",
    "# Creating an instance 'linearReg' for the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('linearReg').getOrCreate()\n",
    "# Reading the data from csv file into the dataframe 'data'\n",
    "data = spark.read.csv('WH_2015.csv', inferSchema=True, header=True)\n",
    "# Printing the schema of the file\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the statistical summary of the data\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays only the features to know what all features are available\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Vectors, VectorAssembler, Pipeline and StringIndexer libraries\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# StringIndexer - A label indexer that maps a string column of labels to an ML column of label indices.\n",
    "# If the input column is numeric, we cast it to string and index the string values.\n",
    "# The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0.\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "# Transforms the input dataset with optional parameters.\n",
    "\n",
    "# A new column 'country_in' is created as an indexer\n",
    "indexer = StringIndexer(inputCol='Country', outputCol='country_in')\n",
    "indexed = indexer.fit(data).transform(data)\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler - A feature transformer that merges multiple columns into a vector column.\n",
    "# The following columns are combined into one feature as 'features'\n",
    "assembler15 = VectorAssembler(inputCols=['country_in',\n",
    " 'Happiness Score',\n",
    " 'Standard Error',\n",
    " 'Economy (GDP per Capita)',\n",
    " 'Family',\n",
    " 'Health (Life Expectancy)',\n",
    " 'Freedom',\n",
    " 'Trust (Government Corruption)',\n",
    " 'Generosity',\n",
    " 'Dystopia Residual',\n",
    " ], outputCol='features')\n",
    "\n",
    "#transform() - Transforms the input dataset with optional parameters and returns transformed dataset.\n",
    "output = assembler15.transform(indexed)\n",
    "final_data=output.select('features', 'Happiness Rank')\n",
    "# randomSplit - Randomly splits this RDD with the provided weights and returns split RDDs in a list. \n",
    "train_data, test_data=final_data.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Linear Regression libraries \n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression - The learning objective is to minimize the squared error, with regularization. \n",
    "# The specific squared error loss function used is: L = 1/2n ||A coefficients - y||^2^\n",
    "# This support multiple types of regularization:\n",
    "#        none (a.k.a. ordinary least squares)\n",
    "#        L2 (ridge regression)\n",
    "#        L1 (Lasso)\n",
    "#        L2 + L1 (elastic net)\n",
    "# regParam - lambda\n",
    "# elasticNetParam - alpha\n",
    "lr=LinearRegression(labelCol='Happiness Rank', maxIter=10, regParam=0.3, elasticNetParam=0.8, \n",
    "                          fitIntercept=True, standardization=True, tol=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit() - Fits a model to the input dataset with optional parameters and returns fitted model.\n",
    "linearmodel = lr.fit(train_data)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = linearmodel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"Happiness Rank\").rdd.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearmodel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept for the model\n",
    "linearmodel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations the model ran\n",
    "linearmodel.summary.totalIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function (scaled loss + regularization) at each iteration.\n",
    "linearmodel.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the residuals of the fitted model by type.\n",
    "linearmodel.summary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RMSE\n",
    "linearmodel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R2\n",
    "linearmodel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean squared error, which is a risk function corresponding \n",
    "#to the expected value of the squared error loss or quadratic loss.\n",
    "linearmodel.summary.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = linearmodel.summary\n",
    "# predictions - Predictions associated with the boundaries at the same index, monotone because of isotonic regression.\n",
    "training_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RegressionEvaluator Libraries for evaluating the Linear Regression model \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Applied the Regression Evaluator to evaluate the label 'Happiness Rank'\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='Happiness Rank')\n",
    "# Evaluation of the predicted values on a scale of 0 to 1\n",
    "evaluator.evaluate(pred_and_labels.predictions,{evaluator.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from csv file into the dataframe 'data'\n",
    "data = spark.read.csv('WH_2016.csv', inferSchema=True, header=True)\n",
    "# Printing the schema of the file\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the statistical summary of the data\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays only the features to know what all features are available\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer - A label indexer that maps a string column of labels to an ML column of label indices.\n",
    "# If the input column is numeric, we cast it to string and index the string values.\n",
    "# The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0.\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "# Transforms the input dataset with optional parameters.\n",
    "\n",
    "# A new column 'country_in' is created as an indexer\n",
    "indexer = StringIndexer(inputCol='Country', outputCol='country_in')\n",
    "indexed = indexer.fit(data).transform(data)\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler - A feature transformer that merges multiple columns into a vector column.\n",
    "assembler16 = VectorAssembler(inputCols=['country_in',\n",
    " 'Happiness Score',\n",
    " 'Lower Confidence Interval',\n",
    " 'Upper Confidence Interval',\n",
    " 'Economy (GDP per Capita)',\n",
    " 'Family',\n",
    " 'Health (Life Expectancy)',\n",
    " 'Freedom',\n",
    " 'Trust (Government Corruption)',\n",
    " 'Generosity',\n",
    " 'Dystopia Residual'\n",
    " ], outputCol='features')\n",
    "\n",
    "#transform() - Transforms the input dataset with optional parameters and returns transformed dataset.\n",
    "output = assembler16.transform(indexed)\n",
    "final_data = output.select('features', 'Happiness Rank')\n",
    "# randomSplit - Randomly splits this RDD with the provided weights and returns split RDDs in a list. \n",
    "train_data, test_data = final_data.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression - The learning objective is to minimize the squared error, with regularization. \n",
    "# The specific squared error loss function used is: L = 1/2n ||A coefficients - y||^2^\n",
    "# This support multiple types of regularization:\n",
    "#        none (a.k.a. ordinary least squares)\n",
    "#        L2 (ridge regression)\n",
    "#        L1 (Lasso)\n",
    "#        L2 + L1 (elastic net)\n",
    "# regParam - lambda\n",
    "# elasticNetParam - alpha\n",
    "lr=LinearRegression(labelCol='Happiness Rank', maxIter=10, regParam=0.3, elasticNetParam=0.8, \n",
    "                          fitIntercept=True, standardization=True, tol=1e-02)\n",
    "\n",
    "# fit() - Fits a model to the input dataset with optional parameters and returns fitted model.\n",
    "linearmodel = lr.fit(train_data)\n",
    "# Generate predictions\n",
    "predicted = linearmodel.transform(test_data)\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"Happiness Rank\").rdd.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearmodel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept for the model\n",
    "linearmodel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations the model ran\n",
    "linearmodel.summary.totalIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function (scaled loss + regularization) at each iteration.\n",
    "linearmodel.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the residuals of the fitted model by type.\n",
    "linearmodel.summary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RMSE\n",
    "linearmodel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R2\n",
    "linearmodel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean squared error, which is a risk function corresponding \n",
    "#to the expected value of the squared error loss or quadratic loss.\n",
    "linearmodel.summary.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = linearmodel.summary\n",
    "# predictions - Predictions associated with the boundaries at the same index, monotone because of isotonic regression.\n",
    "training_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RegressionEvaluator Libraries for evaluating the Linear Regression model \n",
    "from pyspark.ml.evaluation import RegressionEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied the Regression Evaluator to evaluate the label 'Happiness Rank'\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='Happiness Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the predicted values on a scale of 0 to 1\n",
    "evaluator.evaluate(pred_and_labels.predictions,{evaluator.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from csv file into the dataframe 'data'\n",
    "data1 = spark.read.csv('WH_2017.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the schema of the file\n",
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Regular Expression libraries\n",
    "# Substituting '.' to ' ' in all the column names\n",
    "import re\n",
    "data = data1.toDF(*(re.sub(r'[\\.\\s]+', ' ', c) for c in data1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays only the features to know what all features are available\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer - A label indexer that maps a string column of labels to an ML column of label indices.\n",
    "# If the input column is numeric, we cast it to string and index the string values.\n",
    "# The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0.\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "# Transforms the input dataset with optional parameters.\n",
    "\n",
    "# A new column 'country_in' is created as an indexer\n",
    "indexer = StringIndexer(inputCol='Country', outputCol='country_in')\n",
    "indexed = indexer.fit(data).transform(data)\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler - A feature transformer that merges multiple columns into a vector column.\n",
    "assembler17 = VectorAssembler(inputCols=['country_in',\n",
    " 'Happiness Rank',\n",
    " 'Happiness Score',\n",
    " 'Whisker high',\n",
    " 'Whisker low',\n",
    " 'Economy GDP per Capita ',\n",
    " 'Family',\n",
    " 'Health Life Expectancy ',\n",
    " 'Freedom',\n",
    " 'Generosity',\n",
    " 'Trust Government Corruption ',\n",
    " 'Dystopia Residual'\n",
    " ], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform() - Transforms the input dataset with optional parameters and returns transformed dataset.\n",
    "output = assembler17.transform(indexed)\n",
    "final_data = output.select('features', 'Happiness Rank')\n",
    "# randomSplit - Randomly splits this RDD with the provided weights and returns split RDDs in a list. \n",
    "train_data, test_data = final_data.randomSplit([0.8,0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression - The learning objective is to minimize the squared error, with regularization. \n",
    "# The specific squared error loss function used is: L = 1/2n ||A coefficients - y||^2^\n",
    "# This support multiple types of regularization:\n",
    "#        none (a.k.a. ordinary least squares)\n",
    "#        L2 (ridge regression)\n",
    "#        L1 (Lasso)\n",
    "#        L2 + L1 (elastic net)\n",
    "# regParam - lambda\n",
    "# elasticNetParam - alpha\n",
    "lr=LinearRegression(labelCol='Happiness Rank', maxIter=10, regParam=0.3, elasticNetParam=0.8, \n",
    "                          fitIntercept=True, standardization=True, tol=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit() - Fits a model to the input dataset with optional parameters and returns fitted model.\n",
    "linearmodel = lr.fit(train_data)\n",
    "# Generate predictions\n",
    "predicted = linearmodel.transform(test_data)\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"Happiness Rank\").rdd.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearmodel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept for the model\n",
    "linearmodel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations the model ran\n",
    "linearmodel.summary.totalIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function (scaled loss + regularization) at each iteration.\n",
    "linearmodel.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the residuals of the fitted model by type.\n",
    "linearmodel.summary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RMSE\n",
    "linearmodel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R2\n",
    "linearmodel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean squared error, which is a risk function corresponding \n",
    "#to the expected value of the squared error loss or quadratic loss.\n",
    "linearmodel.summary.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = linearmodel.summary\n",
    "# predictions - Predictions associated with the boundaries at the same index, monotone because of isotonic regression.\n",
    "training_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RegressionEvaluator Libraries for evaluating the Linear Regression model \n",
    "from pyspark.ml.evaluation import RegressionEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied the Regression Evaluator to evaluate the label 'Happiness Rank'\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='Happiness Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the predicted values on a scale of 0 to 1\n",
    "evaluator.evaluate(pred_and_labels.predictions,{evaluator.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new instance for SparkSession 'linearReg1'\n",
    "spark = SparkSession.builder.appName('linearReg1').getOrCreate()\n",
    "# Reading data from 3 csv files into respective dataframes \n",
    "df15_sample = spark.read.csv('WH_2015.csv', inferSchema=True, header=True)\n",
    "df16_sample = spark.read.csv('WH_2016.csv', inferSchema=True, header=True)\n",
    "df17_sample = spark.read.csv('WH_2017.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the pyspark sql functions\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the Happiness Rank column to Happiness_Rank_2015 by using alias method\n",
    "df15 = df15_sample.select(\"Country\", \"Region\", col(\"Happiness Rank\").alias(\"Happiness_Rank_2015\"))\n",
    "df15.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the Happiness Rank column to Happiness_Rank_2016 by using alias method\n",
    "df16 = df16_sample.select(\"Country\", \"Region\", col(\"Happiness Rank\").alias(\"Happiness_Rank_2016\"))\n",
    "df16.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Regular Expression libraries\n",
    "# Substituting '.' to ' ' in all the column names\n",
    "import re\n",
    "df17_space = df17_sample.toDF(*(re.sub(r'[\\.\\s]+', ' ', c) for c in df17_sample.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the Happiness Rank column to Happiness_Rank_2017 by using alias method\n",
    "df17 = df17_space.select(\"Country\", col(\"Happiness Rank\").alias(\"Happiness_Rank_2017\"))\n",
    "df17.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the 3 dataframes into one single dataframe by fullouter join\n",
    "df = df15.join((df16.join(df17,['Country'],\"fullouter\")), ['Country','Region'], \"fullouter\").sort(\"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of total Number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the column 'Country' for NULL values\n",
    "df.where(col(\"Country\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the column 'Region' for NULL values\n",
    "df.where(col(\"Region\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer - A label indexer that maps a string column of labels to an ML column of label indices.\n",
    "# If the input column is numeric, we cast it to string and index the string values.\n",
    "# The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0.\n",
    "# Fits a model to the input dataset with optional parameters.\n",
    "# Transforms the input dataset with optional parameters.\n",
    "\n",
    "# A new column 'region_in' is created as an indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer1 = StringIndexer(inputCol='Region', outputCol='Region_in')\n",
    "indexed1 = indexer1.fit(df).transform(df)\n",
    "indexed1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pyspark sql functions and naming them as 'sf'\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# Combining rows which have NULL values for a few columns\n",
    "df_new = indexed1.groupBy(\"Country\", \"Region\", \"Happiness_Rank_2016\")\\\n",
    ".agg(sf.max('Happiness_Rank_2015').alias('Happiness_Rank_2015'), sf.max('Happiness_Rank_2017').alias('Happiness_Rank_2017'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of rows of the new dataframe\n",
    "df_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NULL values with '0'\n",
    "fill_df = df_new.na.fill(0)\n",
    "fill_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. From 2015 to 2017, which country’s happiness ranking increased the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Happiest Country\n",
    "# Adding a new column 'Difference', which shows the ranking difference\n",
    "new_df = fill_df.filter(~(fill_df['Happiness_Rank_2015'] == 0) & ~(fill_df['Happiness_Rank_2016'] == 0) & ~(fill_df['Happiness_Rank_2017'] == 0) & ~(fill_df['Region'] == '0')).withColumn('Difference', fill_df.Happiness_Rank_2015 - fill_df.Happiness_Rank_2017).sort(desc(\"Difference\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. From 2015 to 2017, which country’s happiness ranking decreased the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least Happiest\n",
    "# Adding a new column 'Difference', which shows the ranking difference\n",
    "new_df = fill_df.filter(~(fill_df['Happiness_Rank_2015'] == 0) & ~(fill_df['Happiness_Rank_2016'] == 0) & ~(fill_df['Happiness_Rank_2017'] == 0) & ~(fill_df['Region'] == '0')).withColumn('Difference', fill_df.Happiness_Rank_2015 - fill_df.Happiness_Rank_2017).sort(\"Difference\")\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. For each year, provide the ranking of the happiest continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average of the 'Difference' column and applying groupBy\n",
    "# to the 'Region' column, groups all the continents and gives out the Happiest Continent throughout.\n",
    "final = new_df.filter(~(new_df['Happiness_Rank_2015'] == 0) & ~(new_df['Happiness_Rank_2016'] == 0) & ~(new_df['Happiness_Rank_2017'] == 0) & ~(new_df['Region'] == '0')).groupBy('Region').agg(avg('Difference').alias('Happiest Continent Rank')).sort(desc('Happiest Continent Rank'))\n",
    "final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happiest Continent in 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average of the 'Happiness_Rank_2015' column and applying groupBy\n",
    "# to the 'Region' column, groups all the continents and gives out the Happiest Continent for 2015.\n",
    "final = new_df.filter(~(new_df['Happiness_Rank_2015'] == 0) & ~(new_df['Happiness_Rank_2016'] == 0) & ~(new_df['Happiness_Rank_2017'] == 0) & ~(new_df['Region'] == '0')).groupBy('Region').agg(avg('Happiness_Rank_2015').alias('Happiest Continent in 2015')).sort(desc('Happiest Continent in 2015'))\n",
    "final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happiest Continent in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average of the 'Happiness_Rank_2016' column and applying groupBy\n",
    "# to the 'Region' column, groups all the continents and gives out the Happiest Continent for 2016.\n",
    "final = new_df.filter(~(new_df['Happiness_Rank_2015'] == 0) & ~(new_df['Happiness_Rank_2016'] == 0) & ~(new_df['Happiness_Rank_2017'] == 0) & ~(new_df['Region'] == '0')).groupBy('Region').agg(avg('Happiness_Rank_2016').alias('Happiest Continent in 2016')).sort(desc('Happiest Continent in 2016'))\n",
    "final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Happiest Continent in 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average of the 'Happiness_Rank_2017' column and applying groupBy\n",
    "# to the 'Region' column, groups all the continents and gives out the Happiest Continent for 2017.\n",
    "final = new_df.filter(~(new_df['Happiness_Rank_2015'] == 0) & ~(new_df['Happiness_Rank_2016'] == 0) & ~(new_df['Happiness_Rank_2017'] == 0) & ~(new_df['Region'] == '0')).groupBy('Region').agg(avg('Happiness_Rank_2017').alias('Happiest Continent in 2017')).sort(desc('Happiest Continent in 2017'))\n",
    "final.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
